//////////////install gpu drivers for ubuntu 18.04 LTS////////////////////////////////////////

curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin
sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
sudo add-apt-repository "deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ /"
sudo apt update
sudo apt install cuda
nvidia-smi

//////////////prepare and run benchmarks for BERT///////////////////////////////////////////
#prepare VM for the benchmark code
cd ~
mkdir tmp
cd tmp
wget https://repo.anaconda.com/archive/Anaconda3-2020.02-Linux-x86_64.sh
bash Anaconda3-5_____something
export PATH=~/anaconda3/bin:$PATH
conda --version
conda install -c pytorch pytorch
conda install -c conda-forge transformers
git clone https://github.com/huggingface/transformers
cd transformers
pip install .

#actual benchmark code

from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments
args = PyTorchBenchmarkArguments(models=["bert-base-uncased"], batch_sizes=[1], sequence_lengths=[8, 32, 128, 512])
#args = PyTorchBenchmarkArguments(models=["bert-base-uncased"], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512],no_cuda=True)
benchmark = PyTorchBenchmark(args)
results = benchmark.run()
print(results)


//////////////Install pip3 in VM/////////////////////////////////////////////////////////////////
#install pip3
sudo apt update
sudo apt install python3-pip
pip3 --version

//////////////how to use screen in linux VM/////////////////////////////////////////////////////
type screen
wait untile the virtual screen boots
execute the thread
type ctrl+a then ctrl+d ==== this will ditach the virtual screen
you can now close your ssh
to monitor your results type screen -r to resume the virtual display


////////////configure TPU with VM instance////////////////////////////////////////////////////
first create a compute engine VM

install ctpu tool in the VM machine:
wget https://dl.google.com/cloud_tpu/ctpu/latest/linux/ctpu && chmod a+x ctpu

authenticate operations of adding and deleting TPUs in the VM:
gcloud auth application-default login
and follow the instructions

add ctpu to your path:
sudo cp ctpu /usr/bin/

create a TPU instance, the instance should have the same name as the VM machine (change the project and zone for the next command) (--tpu-only is used because we already have the cpu in the VM, --noconf so that the VM does not ask you for further permissions):
ctpu up --name=tpu-test --project=deep-learning-projects-283921 --zone=europe-west4-a --tpu-size=v3-8  --tpu-only   --tf-version=1.15.dev20190821 --noconf

#for ctpu 2-8
ctpu up --name=tpu-test --project=deep-learning-projects-283921 --zone=us-central1-f --tpu-size=v2-8  --tpu-only   --tf-version=2.3.0.dev20200620 --noconf

see if the tpu is running or not:
ctpu status

delete the tpu and VM instance:
ctpu delete

restart the tpu and cpu:
ctpu restart

for more options use:
ctpu


//////////////Transferring data between Google Drive and Google Cloud Storage using Google Colab/////////////////////////////////////////////////////
#Mount Google Drive in Colab
#Connect to GCS bucket:
from google.colab import auth
auth.authenticate_user()
project_id = '#your_project_id'
!gcloud config set project {project_id}
!gsutil ls
#This will connect to your project and list all buckets
#copy data from or to GCS using gsutil cp command:
bucket_name = '#your_bucket_name'
!gsutil -m cp -r /content/drive/My\ Drive/Data/* gs://{bucket_name}/
#If you copy more than a few files, use the -m option for gsutil, as it will enable multi-threading and speed up the copy process significantly.

////////////////////////////making create_pretraining_data.py work in bert pretraining/////////////////////////////////////
this code will always give you OOM error if you are just shoving the whole dataset in one go
to solve this problem you should partition your data into smaller files each with 250000 lines and then run the code in each file with a different output file. see the command below

python3 create_pretraining_data.py \
  --input_file=gs://sudabert_bucket/sudanese_txt_cleaned_shuffled_splits/suda_txt_1.txt \
  --output_file=gs://sudabert_bucket/tmp/tf_examples.tfrecord_1 \
  --vocab_file=gs://sudabert_bucket/ar_bert/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5

when you run the pretraining the input file will be gs://sudabert_bucket/tmp/ using the following command 

  python3 run_pretraining.py \
  --input_file=gs://sudabert_bucket/tmp/* \
  --output_dir=gs://sudabert_bucket/pretraining_output \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=gs://sudabert_bucket/ar_bert/config.json \
  --init_checkpoint=gs://sudabert_bucket/ar_bert/ar_bert_base_uncased.ckpt \
  --train_batch_size=32 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=100000 \
  --num_warmup_steps=1000 \
  --learning_rate=2e-5 \
  --use_tpu=True \
  --tpu_name=bert-tpu-pretraining

the above commands should be run in bert tensorflow repo i.e. the commands should be run inside the bert file downloaded with the command:
git clone https://github.com/google-research/bert.git


////////////////////////////converting bert original tf checkpoint to pytorch(.bin)/////////////////////////////////////
This CLI takes as input a TensorFlow checkpoint (three files (data, index, meta) starting with bert_model.ckpt) and the associated configuration file (bert_config.json), and creates a PyTorch model for this configuration.
code: https://github.com/huggingface/transformers/blob/master/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
just take the convert_tf_checkpoint_to_pytorch function and use it.
convert_tf_checkpoint_to_pytorch(path_to_model.ckpt, path_to_config.json, path_to_the_file_where_you_want_to_save_model.bin)


////////////////////////////finetune pretrained bert (pretraining files are not published in huggingface surver) on sequence classification/////////////////////////////////////
1. this requires (model.bin, config.lson, vocab.txt), so firstly, convert tf checkpoint to pytorch.
2. follow this notebook: https://github.com/aub-mind/arabert/blob/master/examples/AraBERT_Text_Classification_with_HF_Trainer_Pytorch_GPU.ipynb
until reaching the cell:

config = AutoConfig.from_pretrained(model_name,num_labels=num_labels, output_attentions=True) ##needed for the visualizations
tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False, do_basic_tokenize=True, never_split=never_split_tokens)
model = BertForSequenceClassification.from_pretrained(model_name,config=config)

and change it to:

config = PretrainedConfig.from_pretrained(path_to_config.json,num_labels=num_labels, output_attentions=True) ##needed for the visualizations
tokenizer = BertTokenizer.from_pretrained(path_to_vocab.txt, do_lower_case=False, do_basic_tokenize=True, never_split=never_split_tokens)
model = BertForSequenceClassification.from_pretrained(path_to_model.bin, config=config)

3. you can skip the tensorboard and visualization part at the end of the notebook but:
you have to run trainer.evaluate() after training (to get the test-set results).


////////////////////////////Convert Pytorch checkpoint to Tensorflow checkpoint/////////////////////////////////////
if the pytorch model is saved in huggingface servers, follow this notebook:
https://colab.research.google.com/drive/1BDEnjU9qE2brp36NC4XF0pbcfNEZNFWd?usp=sharing

if it is saved locally, run this code:
https://github.com/huggingface/transformers/blob/master/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py

////////////////////////////change to python 3.x /////////////////////////////////////////////////
sudo apt-get update
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt-get update
sudo apt-get install python3.x

sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.x 2
sudo update-alternatives --config python3

//////////////////////////////////install tensorflow 1.12 /////////////////////////////////////////////////////
pip3 install tensorflow-gpu==1.12.0
pip3 install tensorflow==1.12.0


////////////////////////////////////notes some errors ///////////////////////////////////////////////////////
when facing segmentation fault or killed this probably due to insufficient ram memory

///////////////////////////////////configure GCP TPU for pytorch/xla ///////////////////////////////////////
create a VM with boot image debian-9-torch-xla-v20200821 (can be found under category Deep Learning for Linux) check the box Allow full access to all Cloud APIs in Access scopes

create the TPU using gcloud command:
 gcloud compute tpus create distilbert-pretraining \
--zone=europe-west4-a \
--network=default \
--version=pytorch-1.6 \
--accelerator-type=v3-8

then check its IP address using:
gcloud compute tpus list --zone=europe-west4-a

activate conda enviroment named torch-xla-nightly:
conda activate torch-xla-nightly

add TPU configuration to the VM path (replace $IP_ADDRESS$ with the ip address of your TPU):
export XRT_TPU_CONFIG="tpu_worker;0;$IP_ADDRESS$:8470"

check if this worked:
python3 and then
import torch
import torch_xla
import torch_xla.core.xla_model as xm
dev = xm.xla_device()
t1 = torch.ones(3, 3, device = dev)
print(t1)


////////////////////////////////////fine-tuning ///////////////////////////////////////////////////////
Sentiment Analysis:
AJGT : https://colab.research.google.com/drive/13F8pk6shiap69LPi8pqMBhoEXsyJZGzP?usp=sharing
ArSenTD-LEV : https://colab.research.google.com/drive/1shm1xEvVlbq3Kp8jQtrRjm9Y2QHOYB8N?usp=sharing
ASTD : https://colab.research.google.com/drive/1f7tIbqrK_dIOZEVB4sK-Qwbf02hpPO6j?usp=sharing
HARD : https://colab.research.google.com/drive/1uX7yRs5NZccMS0san4qY4v7dTn_JXkxL?usp=sharing
LABR : https://colab.research.google.com/drive/18ycYrARov_SMUsxqDfJwwcOiRvYaUQ-h?usp=sharing

Named Entity Recognition:
ANERCorp : https://colab.research.google.com/drive/1qq_EbosfZuTFNB_zV42hF357TuX5C9pq?usp=sharing

Make sure this folder is added to your drive : https://drive.google.com/drive/folders/1_RmxaCi6fBAYEns-m2FsWDuds8moIXM_?usp=sharing
It contains our pretrained model, config and vocab files.


///////////////////////////Qeustion Answering ////////////////////////////////////////////////////
follow the the instructions on QA_vx.ipynb 


//////////////////////////telegram bot////////////////////////////////////////////////////////////
first download gpt2-xl:
pip install regex==2017.4.5
git clone https://github.com/graykode/gpt-2-Pytorch
cd gpt-2-Pytorch
curl --output gpt2-xl-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-pytorch_model.bin

then to use it with cmd copy the script simulate_conv_gpt_xl.py into gpt-2-Pytorch and run it.

to run the bot in telegram do the following:
(taken from the tutorail https://medium.com/jj-innovative-results/how-to-create-a-simple-telegram-bot-in-python-using-nginx-and-gcp-926f1b0fb16f )

-update everything:
sudo apt update -y && sudo apt-get update -y && sudo apt-get upgrade -y && sudo apt dist-upgrade -y && sudo apt-get autoremove -y && sudo apt-get clean -y && sudo apt-get autoclean -y

-intall telegram api and other dependencies:
sudo apt-get install software-properties-common build-essential cmake git wget curl mosh vim nginx python3-pip && pip3 install python-telegram-bot

-create private and public keys (you can choose any names for private and public keys, the EXTERNAL IP OF VM INSTANCE should be replaced with the IP of your instance):
openssl req -newkey rsa:2048 -sha256 -nodes -keyout PRIVATE.key -x509 -days 365 -out PUBLIC.pem -subj “/C=US/ST=STATE/L=CITY/O=BUSINESS NAME/CN=EXTERNAL IP OF VM INSTANCE”

-then move those files to /etc/nginx:
sudo mv PUBLIC.pem /etc/nginx
sudo mv PRIVATE.key /etc/nginx

-create a telegram bot by searching for BotFather and create a bot, the BotFather will give you access token (TELEGRAMTOKEN) that we will use. 

-create the config file (named TEXT here) for nginx () using command cd /etc/nginx/sites-available/ && sudo vim TEXT.txt and fill it with the following code (you should replace PUBLIC,private with the name you choose for the public,private keys also you should replace EXTERNAL IP OF VM INSTANCE with your VM ip and replace TELEGRAMTOKEN with the access token of your bot make sure to replace it in both places):
server {
  listen 80;
  listen 443 ssl;
  server_name EXTERNAL IP OF VM INSTANCE;
  ssl_certificate /home/jj/PUBLIC.pem;
  ssl_certificate_key /home/jj/PRIVATE.key;

  location /TELEGRAMTOKEN {
    proxy_set_header Host $http_host;
    proxy_redirect off;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Scheme $scheme;
    proxy_pass http://0.0.0.0:5000/TELEGRAMTOKEN/;
    }
}

-then cd back from those files (cd ~)

-activate this configrations:
sudo ln -s /etc/nginx/sites-available/TEXT.txt /etc/nginx/sites-enabled/TEXT.txt

-activate nginx (if every thing go well you should see that nginx is active (running)):
sudo systemctl restart nginx && sudo systemctl status nginx

-then use the file telegram_sim_conv_gpt_xl.py located in gpt file in this repo and move it to gpt-2-Pytorch, open telegram_sim_conv_gpt_xl.py and change TOKEN to your telegram token, server to the IP address of your VM and CERT to the path of the PUBLIC.pem (/etc/nginx/PUBLIC.pem)

-everything is now ready just run telegram_sim_conv_gpt_xl.py and try out sending messages to your bot.

